# airflow
Apache Airflow DAGs for data processing, reporting, and anomaly detection.

Данный репозиторий содержит все DAG Apache Airflow, используемые для различных задач по работе с данными и операционных задач. В частности, здесь вы найдете:

*   **ETL (Extract, Transform, Load) процессы:** DAG, созданные для регулярного извлечения и преобразования данных в соответствии с бизнес-логикой, загрузки в целевые системы и дальнейшего использования для анализа и отчетности. 
*   **Отправка ключевых метрик в бот в реальном времени:** DAG, отвечающие за извлечение ключевых показателей эффективности из источников данных и их запланированную отправку в назначенные боты. Это гарантирует, что заинтересованные стороны будут в курсе наиболее релевантных метрик.
*   **Обнаружение аномалий и оповещения:** DAG, которые отслеживают данные на наличие аномалий или необычных закономерностей в режиме реального времени. При обнаружении каких-либо аномалий эти DAG запускают оповещения для соответствующих команд для оперативного расследования и устранения проблем.

Эти DAG предназначены для автоматизации обработки данных, обеспечения своевременной доставки ключевых метрик и проактивного уведомления команд о потенциальных проблемах. 


# Проект 1: ETL-процесс для анализа пользовательской активности

**Задача:**

Разработать **автоматизированный** ETL-процесс для сбора, обработки и анализа данных о пользовательской активности в виде действий с контентом (feed) и сообщениями (messages), обеспечивающий интеграцию с Redash для дальнейшей визуализации и построения дашбордов.

**Функциональные требования:**

1.  **Сбор данных:**
    *   Автоматическое извлечение данных из источников, содержащих информацию о действиях пользователей в ленте (`feed_actions`) и мессенджере (`message_actions`).
    *   Сбор данных должен выполняться ежедневно за предыдущий день.

2.  **Обработка и трансформация данных:**
    *   Агрегация данных на уровне пользователя для подсчета следующих метрик:
        *   Количество просмотров (`views`) контента.
        *   Количество лайков (`likes`) контента.
        *   Количество полученных сообщений (`messages_received`).
        *   Количество отправленных сообщений (`messages_sent`).
        *   Количество уникальных пользователей, от которых получены сообщения (`users_received`).
        *   Количество уникальных пользователей, которым отправлены сообщения (`users_sent`).
    *   Агрегация данных должна выполняться в разрезах по следующим демографическим признакам:
        *   Операционная система пользователя (`os`).
        *   Пол пользователя (`gender`).
        *   Возраст пользователя (`age`).

3.  **Структура итоговой таблицы:**

    Результирующие данные должны быть структурированы в виде таблицы со следующими полями:

    | Название поля       | Тип данных | Описание                                                                     |
    | :------------------ | :--------- | :--------------------------------------------------------------------------- |
    | `event_date`         | Date       | Дата события, за которую собраны данные.                                     |
    | `dimension`         | String     | Название среза: "os", "gender" или "age".                                  |
    | `dimension_value`   | String     | Значение среза: например, "iOS", "male", "25".                            |
    | `views`             | Integer    | Число просмотров контента.                                                   |
    | `likes`             | Integer    | Число лайков контента.                                                       |
    | `messages_received` | Integer    | Число полученных сообщений.                                                  |
    | `messages_sent`     | Integer    | Число отправленных сообщений.                                               |
    | `users_received`    | Integer    | Число уникальных пользователей, от которых получены сообщения.              |
    | `users_sent`        | Integer    | Число уникальных пользователей, которым были отправлены сообщения.            |

4. **Загрузка данных:**
   * Ежедневная автоматическая загрузка обработанных и агрегированных данных в целевую таблицу ClickHouse.
   * Таблица должна пополняться новыми данными каждый день.

5.  **Интеграция с Redash:**
    *   Обеспечить возможность подключения итоговой таблицы к Redash для визуализации данных, построения дашбордов и анализа.

**Технические требования:**

*   **Автоматизация:** Весь процесс ETL должен быть автоматизирован и запускаться ежедневно.
*   **Технологии:**
    *   Использовать Apache Airflow для оркестрации ETL-процесса.
    *   Использовать ClickHouse в качестве хранилища данных.
    *   Использовать Redash для визуализации.

## Описание ETL процесса

**Этапы процесса:**

1.  **Извлечение (Extract):**
    *   **Автоматическая** параллельная выгрузка данных из таблиц `feed_actions` и `message_actions` ClickHouse за предыдущий день.
2.  **Трансформация (Transform):**
    *   Объединение данных.
    *   Агрегация метрик.
    *   Расчет метрик по срезам.
3.  **Загрузка (Load):**
    *   **Автоматическая** ежедневная загрузка агрегированных данных в целевую таблицу ClickHouse.

## Визуализация данных

**Интеграция с Redash:**

Созданная таблица автоматически загружается в ClickHouse, подключенный к Redash. Это позволяет:

*   Использовать Redash для создания динамических дашбордов, отображающих метрики вовлеченности пользователей в реальном времени.
*   Создавать визуализации для отслеживания трендов, выявления аномалий и анализа эффективности различных стратегий.

## Визуализация ETL-процесса в Airflow

Ниже представлен общий вид DAG в Apache Airflow:

![dag_project1](https://github.com/user-attachments/assets/f486153c-4a65-426b-bc8a-731d9c068bb8)

## Код

Ознакомится с кодом можно в "dag_project1.py"


## Проект 2: Автоматизированная аналитическая отчетность в Telegram

**Задача:**

Создать автоматизированную ежедневную отправку аналитической сводки о работе приложения в Telegram. Отчет должен обеспечить заинтересованных лиц актуальной информацией по ключевым метрикам в удобном для просмотра формате.

**Функциональные требования:**

1.  **Метрики:**
    Отчет должен содержать следующие ключевые метрики, рассчитанные за предыдущий день:
    
    *   **DAU (Daily Active Users):**
        *   Количество уникальных пользователей, взаимодействовавших с **Лентой новостей** и с **Мессенджером**.
    *   **Лайки (Likes) и Просмотры (Views):**
        *   Общее количество лайков и просмотров контента в **Ленте новостей**.
    *   **CTR (Click-Through Rate):**
        *   Отношение лайков к просмотрам в **Ленте новостей**.
        * **Отправленные сообщения (Messages Sent)**:
           * Общее количество отправленных сообщений в **Мессенджере**.
        *  **Отправлено/Получено**:
           * Соотношение отправленных и полученных сообщений в **Мессенджере**.   

2.  **Отображение динамики метрик (7 days offset):**
    *   Для каждой из вышеуказанных метрик отчет должен включать графики, отображающие динамику метрик за последние 7 дней (с учетом предыдущего дня).
    *   Графики должны быть включены в отчет в виде изображений.
    *   Графики должны отображать отдельно метрики для Ленты и для Мессенджера.
    
3.  **Формат и содержание отчета:**
    *  Отчет должен отправляться как сообщение в телеграм, включая текст с значениями метрик и изображения графиков.
    *  Отчет должен предоставлять информацию об общей динамике приложения, а также о его отдельных частях (Ленте и Мессенджере).
    *  Отчет должен содержать ссылку на дашборд в Superset, для более детальной информации о метриках. 

4.  **Время отправки отчета:**
    *   Отчет должен отправляться ежедневно в 11:00 (утра) по местному времени.
    *   Отправка отчета должна быть полностью автоматизирована и не требовать ручного вмешательства.

**Описание решения:**

Для достижения поставленной цели необходимо реализовать следующий ETL-процесс и набор задач:

1.  **Подготовка Telegram-бота:**
    *   Создание Telegram-бота с использованием @BotFather.
    *   Получение `chat_id` для отправки сообщений в нужный чат или пользователю.

2.  **Сбор и агрегация данных:**
    *   Разработка скрипта для сбора данных о метриках приложения, вычисления метрик за предыдущий день и предшествующие 7 дней.

3.  **Формирование отчета:**
    *   Формирование текстового отчета со значениями ключевых метрик за предыдущий день.
    *   Создание набора графиков с динамикой метрик за последние 7 дней, объединение их на одном изображении с помощью 'subplots'. 
    *   Изображение с графиками сохранялось на в **формате PNG** для дальнейшей отправки.

4.  **Интеграция с Apache Airflow:**
    *   Реализация ETL-процесса с использованием Apache Airflow для автоматизации сбора данных, формирования отчета и его отправки.
    *   Разработка DAG (Directed Acyclic Graph), который будет запускать процесс по расписанию.

## Визуализация процесса в Airflow

Ниже представлен общий вид DAG в Apache Airflow:

![dag_project2](https://github.com/user-attachments/assets/519846e4-e8c5-4364-8d46-80a0f9914960)

## Код

Ознакомится с кодом можно в "dag_project2.py"

## Результат работы DAG: Отчет в Telegram

Ниже показан пример отчета, отправляемого в Telegram:

![dag_project2_2](https://github.com/user-attachments/assets/f613a9f3-4a90-4ffb-9372-f9d22b699567)


**Технологии:**

*   Python
*   Apache Airflow
