# airflow
Apache Airflow DAGs for data processing, reporting, and anomaly detection.

Данный репозиторий содержит все DAG Apache Airflow, используемые для различных задач по работе с данными и операционных задач. В частности, здесь вы найдете:

*   **ETL (Extract, Transform, Load) процессы:** DAG, созданные для регулярного извлечения и преобразования данных в соответствии с бизнес-логикой, загрузки в целевые системы и дальнейшего использования для анализа и отчетности. 
*   **Отправка ключевых метрик в бот в реальном времени:** DAG, отвечающие за извлечение ключевых показателей эффективности из источников данных и их запланированную отправку в назначенные боты. Это гарантирует, что заинтересованные стороны будут в курсе наиболее релевантных метрик.
*   **Обнаружение аномалий и оповещения:** DAG, которые отслеживают данные на наличие аномалий или необычных закономерностей в режиме реального времени. При обнаружении каких-либо аномалий эти DAG запускают оповещения для соответствующих команд для оперативного расследования и устранения проблем.

Эти DAG предназначены для автоматизации обработки данных, обеспечения своевременной доставки ключевых метрик и проактивного уведомления команд о потенциальных проблемах. 


# Проект 1: ETL-процесс для анализа пользовательской активности

**Задача:**

Разработать **автоматизированный** ETL-процесс для сбора, обработки и анализа данных о пользовательской активности в виде действий с контентом (feed) и сообщениями (messages), обеспечивающий интеграцию с Redash для дальнейшей визуализации и построения дашбордов.

**Функциональные требования:**

1.  **Сбор данных:**
    *   Автоматическое извлечение данных из источников, содержащих информацию о действиях пользователей в ленте (`feed_actions`) и мессенджере (`message_actions`).
    *   Сбор данных должен выполняться ежедневно за предыдущий день.

2.  **Обработка и трансформация данных:**
    *   Агрегация данных на уровне пользователя для подсчета следующих метрик:
        *   Количество просмотров (`views`) контента.
        *   Количество лайков (`likes`) контента.
        *   Количество полученных сообщений (`messages_received`).
        *   Количество отправленных сообщений (`messages_sent`).
        *   Количество уникальных пользователей, от которых получены сообщения (`users_received`).
        *   Количество уникальных пользователей, которым отправлены сообщения (`users_sent`).
    *   Агрегация данных должна выполняться в разрезах по следующим демографическим признакам:
        *   Операционная система пользователя (`os`).
        *   Пол пользователя (`gender`).
        *   Возраст пользователя (`age`).

3.  **Структура итоговой таблицы:**

    Результирующие данные должны быть структурированы в виде таблицы со следующими полями:

    | Название поля       | Тип данных | Описание                                                                     |
    | :------------------ | :--------- | :--------------------------------------------------------------------------- |
    | `event_date`         | Date       | Дата события, за которую собраны данные.                                     |
    | `dimension`         | String     | Название среза: "os", "gender" или "age".                                  |
    | `dimension_value`   | String     | Значение среза: например, "iOS", "male", "25".                            |
    | `views`             | Integer    | Число просмотров контента.                                                   |
    | `likes`             | Integer    | Число лайков контента.                                                       |
    | `messages_received` | Integer    | Число полученных сообщений.                                                  |
    | `messages_sent`     | Integer    | Число отправленных сообщений.                                               |
    | `users_received`    | Integer    | Число уникальных пользователей, от которых получены сообщения.              |
    | `users_sent`        | Integer    | Число уникальных пользователей, которым были отправлены сообщения.            |

4. **Загрузка данных:**
   * Ежедневная автоматическая загрузка обработанных и агрегированных данных в целевую таблицу ClickHouse.
   * Таблица должна пополняться новыми данными каждый день.

5.  **Интеграция с Redash:**
    *   Обеспечить возможность подключения итоговой таблицы к Redash для визуализации данных, построения дашбордов и анализа.

**Технические требования:**

*   **Автоматизация:** Весь процесс ETL должен быть автоматизирован и запускаться ежедневно.
*   **Технологии:**
    *   Использовать Apache Airflow для оркестрации ETL-процесса.
    *   Использовать ClickHouse в качестве хранилища данных.
    *   Использовать Redash для визуализации.

## Описание ETL процесса

**Этапы процесса:**

1.  **Извлечение (Extract):**
    *   **Автоматическая** параллельная выгрузка данных из таблиц `feed_actions` и `message_actions` ClickHouse за предыдущий день.
2.  **Трансформация (Transform):**
    *   Объединение данных.
    *   Агрегация метрик.
    *   Расчет метрик по срезам.
3.  **Загрузка (Load):**
    *   **Автоматическая** ежедневная загрузка агрегированных данных в целевую таблицу ClickHouse.

## Визуализация данных

**Интеграция с Redash:**

Созданная таблица автоматически загружается в ClickHouse, подключенный к Redash. Это позволяет:

*   Использовать Redash для создания динамических дашбордов, отображающих метрики вовлеченности пользователей в реальном времени.
*   Создавать визуализации для отслеживания трендов, выявления аномалий и анализа эффективности различных стратегий.

## Визуализация ETL-процесса в Airflow

Ниже представлен общий вид DAG в Apache Airflow:

![dag_project1](https://github.com/user-attachments/assets/f486153c-4a65-426b-bc8a-731d9c068bb8)

## Код

Ознакомится с кодом можно в "dag_project1.py"
